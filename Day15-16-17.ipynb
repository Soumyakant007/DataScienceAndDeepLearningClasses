{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3e88ecc9-19c1-46d4-8f7e-5d81e52dd40f",
   "metadata": {},
   "source": [
    "# Deep Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf82bca5-dc61-408c-8c87-3e642040fc9b",
   "metadata": {},
   "source": [
    "The subset of machine learning  composed of algorithms that permits software to train itself, to perform some tasks, like speech recognition and image recognition by exposing multilayered neural network to vast amount of data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72093d70-a039-456a-a238-cae66c559ace",
   "metadata": {},
   "source": [
    "Neural network consists of neurons. These neurons are connected to each other to form a network. \n",
    "That network contains an input layer, a hidden layer and a output later"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7b8c5b8-a381-4ed1-850e-ef946d50e872",
   "metadata": {},
   "source": [
    "- Deep network composed of artificial neuron\n",
    "- Activation function is usually Sigmoud and also can be tanh and ReLu\n",
    "- The method used to train a neural network is called backpropagation\n",
    "- Traditional neural network with all signals propagating to one direction is called feedforward network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba2ab46c-d84c-4055-9022-b7d9ce488fed",
   "metadata": {},
   "source": [
    "Important property of neural network is\n",
    "- We get better result with\n",
    "     - More data\n",
    "     - Bigger models\n",
    "     - more computation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfb25c11-b6a5-4ce8-b504-7a68c40098a5",
   "metadata": {},
   "source": [
    "## TensorFlow\n",
    "Tensorflow is an open source software library for numerical computation using dataflow graphs. Nodes in the graph represent mathematical operations, while the graph edges represent the multidimensional data arrays (Tensors) communicated between them. The flexible architecture allows you to deploy computation to one or more cpu or gpus in a desktop server or mobile device with a single api."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1813b25e-764c-4f46-9f29-f5a3ca5257c5",
   "metadata": {},
   "source": [
    "## Keras\n",
    "It is minimalistic python library for deep learning that can run on top of Theiano and TensorFlow. It was developed to make implementing deep learning models on fast and easy as portable for research andÂ development."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "269b84ed-0a75-457c-a6fe-7def65dea10b",
   "metadata": {},
   "source": [
    "Si=Sum(X t-a * W -a)(i=0->n)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29535557-01f2-4aba-afe2-efae1641fc1f",
   "metadata": {},
   "source": [
    "- We can think of images as 2d inputs\n",
    "- We would now like to use a 2d fiter(mxn)\n",
    "- 2d formula:\n",
    "- - Sij=(I*K)ij=Sum(Sum(I(i-a.j-b) * K(a.b)(b=0->n-1))(a=0->m-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be038816-bc2d-47a7-a541-747f445b615f",
   "metadata": {},
   "source": [
    "Ex \n",
    "- Image * [[1,1,1],[1,1,1],[1,1,1]]= Blur image\n",
    "- Image * [[0,-1,0],[-1,-5,-1],[0,-1,0]] = Sharpen Image\n",
    "- Image * [[1,1,1],[1,-8,1],[1,1,1]] = Blackend"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f2ee41b-fca5-446e-857a-3639e50d0f27",
   "metadata": {},
   "source": [
    "A CNN can be implemented as a feed forwardf neural network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "058ecb95-5d84-4557-8059-31c37bd3c2a5",
   "metadata": {},
   "source": [
    "- input=[[b,c,d],\n",
    "       [e,f,g],\n",
    "       [h,i,j]]\n",
    "- kernel(weight)=[[w,x],\n",
    "        [y,z]]\n",
    "- output=\n",
    "\n",
    "We can thus train a convolution neural network by thinking of it  as a feed forward neural network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd64328b-6557-48d7-925d-5b7f11f22f42",
   "metadata": {},
   "source": [
    "- Deep Learning: neural network with several layers of node between input and output\n",
    "- Why better: the series of layers betwenn i/o do feature identification and processing in a series of stages just as out brain seem to"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfc5f5bf-539c-400e-bcfb-25ef2ba3f1ef",
   "metadata": {},
   "source": [
    "We have always had good algorithms for learning the weights in network with 1 hidden layer, but these algorithms are not good ar learning the weights for network with more hidden layer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d575163-a021-46f2-8428-28213d64cc99",
   "metadata": {},
   "source": [
    "Activation functions:\n",
    "- Linear\n",
    "- Sigmoid\n",
    "- Tanh\n",
    "- ReLu\n",
    "- Softmax"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6812f44-e34e-4052-b49a-275673058d1b",
   "metadata": {},
   "source": [
    "- weight-learning algorithms try to make tiny changes to parameters to be better than nearest value, bubt worse than values furtehr from it\n",
    "- If f(xis linear, the neural network can only draw straight decision boundaries even if there are many layers of unit\n",
    "- Itf it uses non linear f(x), they can draw complex boundaries, but keep the data unchanged\n",
    "- SVMs only draw straight lines, but they transform the data first in a way that makes that OK\n",
    "\n",
    "- Inside hidden layer, we have self organised feature detector"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2154cfdc-f02e-4229-af6e-2c9d14dbf4d3",
   "metadata": {},
   "source": [
    "many-layer neural network architechtucre should be capab;le of learning the true underlying feature and featurelogic"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ba81580-416a-4cc8-9f39-46955c376a41",
   "metadata": {},
   "source": [
    "Until recently weighted learning were not possible to implement in multi-layer NN.\n",
    "So we do transfer learning, first 2 layers are first trained, then the 2nd and 3rd layer, then the 3rd and the 4th layer....like this all layers are trained.\n",
    "Each of the (non-output) layer is trained to be an auto encoder. Basically it is forced to learn good features that comes from the previous layer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eedf4b08-850f-424c-aa10-f54d25d70231",
   "metadata": {},
   "source": [
    "An auto encoder is trained with an absolutely standard weight adjustment algorithm to reproduce the input. Each intermediate layers are trained top be auto encoders or similar"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff7dcfd7-b727-473b-af73-ef311fb89586",
   "metadata": {},
   "source": [
    "CIFAR 10\n",
    "\n",
    "\n",
    "### Deep learning with autoencoders\n",
    "-  Logistic Regression\n",
    "-  Sparse encoders\n",
    "-  Neural network\n",
    "-  Deep autoencoder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f84cc6b1-701a-46aa-b02d-b75e0c630ac7",
   "metadata": {},
   "source": [
    "## Scaling up with GPU"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f85e0d86-797a-4d5f-81e5-8d7c4bad9991",
   "metadata": {},
   "source": [
    "#### Why GPU?\n",
    "- Every set of weights can be  stored as a matrix\n",
    "- GPUs are made to do common paralle problems fast, all similar calculations are done in same time\n",
    "\n",
    "#### Why is GPU different from CPU\n",
    "- Assumes workload is highly parallel\n",
    "- CPU must be good at everything, parallel or not\n",
    "\n",
    "  \n",
    "\n",
    "- CPU: minimize latency experienced by 1 thread\n",
    "    - big on-chip caches\n",
    "    - sophisticated control logic\n",
    "\n",
    "- GPU: maximize throughput of all threads\n",
    "     - threads in flight limited by measures => lots of resources\n",
    "     - Multithreading can hide latency => skips big caches\n",
    "     - Share control logic of many threads"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73642d75-b634-405a-8e20-d16252aecccc",
   "metadata": {},
   "source": [
    "## Recommmender System:\n",
    "Software tools and techniques providing suggestions for items to be of use to a user\n",
    "\n",
    "### Colaborative filtering with deep learning\n",
    "\n",
    "### Matrix Factorinzation\n",
    "m users and n items, each has p features user data U is a matrix with size m^p, Item data V is a matrix of size n^p\n",
    "\n",
    "For user u belongs U and v belongs to V Predicting rating rij= ui * ujT\n",
    "Error=ui * vjT -Rij\n",
    "\n",
    "\n",
    "#### Limitation of Colaborative filtering\n",
    "- Cold start problem"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ca60e76-aa7c-436d-8589-afda2313bcea",
   "metadata": {},
   "source": [
    "- user latent\n",
    "- item latent \n",
    "- task features\n",
    "- visual features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30af1383-06d8-4950-a2e0-af5107ffff42",
   "metadata": {},
   "source": [
    "### Text Feature Learning\n",
    "#### Stack fully-connected autoencoder\n",
    "- low level to compressed high level feature vector\n",
    "\n",
    "#### Stack convolutional autoencoder\n",
    "- low level to high level compressed feature vector by using convolution of different layers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2224b44f-0eca-45e2-a70d-11733cec56da",
   "metadata": {},
   "source": [
    "## Summary\n",
    "- A hybrid recommender system with integreated collaborative filtering and deep learning has been implemented\n",
    "- With extra features from text and images, there systems outperform the traditional recommender system"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3ae2d4d-673e-413d-8875-a45ebd55471b",
   "metadata": {},
   "source": [
    "Deep learning = Learning hierarchical representation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88fe72cc-a23f-474b-88a6-2a63cfcf3ec8",
   "metadata": {},
   "source": [
    "A neural network is a computataional unit in the neural network that exchange messages with eachother"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db4ba923-d98e-458e-a065-161e16d05d82",
   "metadata": {},
   "source": [
    "Common deeplearnign model\n",
    "\n",
    "- resize image\n",
    "- run concolutional network\n",
    "- non max suppression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d89ea7f-7107-4589-8432-3709fe1e44a6",
   "metadata": {},
   "source": [
    "### Image to text\n",
    "- input image\n",
    "- convolutional feature extraction\n",
    "- RNN with attention over the image\n",
    "- word by word generation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f864515-e4c6-4a5b-a397-f86244a29b11",
   "metadata": {},
   "source": [
    "#### Text to image\n",
    "- Caption->Project and reshape->deconv1->deconv2->...->Image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b6471e3-75af-4e14-b34c-58540a7d0bb2",
   "metadata": {},
   "source": [
    "Feed forward:\n",
    "- Activate the neuron from bottom to the top\n",
    "\n",
    "Back propagation\n",
    "- Randomly initializa the parameters\n",
    "- Calculate total error at the top\n",
    "- Then calculate contribution to error , at each step going backwards"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a2f952f-5138-4aa5-be7e-de741bae74af",
   "metadata": {},
   "source": [
    "## Limits\n",
    "- High cost\n",
    "   - Each neuron considered as logistic regression\n",
    "   - Training the entire neural network is to train all the internconnected logistics regression\n",
    "- Difficult to train as the number of layer increases\n",
    "- Stuck in local optima\n",
    "- Deep learning/Learning multiple level of represemtation      "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f9f2b0d-0275-42ab-ba5a-14d85fa40b82",
   "metadata": {},
   "source": [
    "### Deep Belief Network\n",
    "- is a probabilistic, generative model made up of multiple layers of hidden units\n",
    "- used to generatively pre-train a Deep neural network by using the learned DBN weights as the initial DNN weights\n",
    "#### Resticted Boltzmann Machine\n",
    "- A DBN can be efficiently trained in an unsuoervised learning, layer-by-layer manner, where the layeras are made of RBM\n",
    "#### Contrastive Divergence\n",
    "- provides an approximation to the maximum likelihood method method that would ideally be applied for learning the weights of the RBM with gradient ascent\n",
    "- \n",
    "\n",
    "#### Autoencoders"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa1439f3-70ab-45ec-a684-7df156d29436",
   "metadata": {},
   "source": [
    "#### Convolutional Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f24ed59-ce86-497a-a9bd-192c102c6a50",
   "metadata": {},
   "source": [
    "# Summary\n",
    "\n",
    "TOday we learned about, Deep learning, which involves training algorithms, like neural networks with layers of interconnected neurons, to perform tasks such as speech and image recognition through exposure to extensive data. Key components include activation functions like Sigmoid and ReLU, training via backpropagation, and tools like TensorFlow and Keras for implementation.  Challenges include optimizing network performance with more data and computational resources. Techniques like CNNs and autoencoders further refine deep learning applications, enhancing capabilities in image processing and pattern recognition tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2e0ec5b-635e-47ef-bc8e-69686025a143",
   "metadata": {},
   "source": [
    "Deep Learning involves training neural networks with multiple layers (input, hidden, and output) using vast data. Activation functions like Sigmoid, Tanh, and ReLU transform neuron outputs. Backpropagation adjusts weights to minimize errors. Feedforward networks pass signals in one direction. TensorFlow and Keras aid in model implementation. Convolutional Neural Networks (CNNs) process 2D data (e.g., images) with filters. GPUs enhance training by parallelizing computations. Autoencoders and deep belief networks facilitate feature learning and weight initialization. Recommender systems combine deep learning and collaborative filtering, improving with extra features. Deep learning excels with more data, larger models, and greater computation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e48fe2e2-a1c8-4b33-8a31-3e738c2e5adf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
