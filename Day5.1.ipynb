{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4ba8530b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b90d3edb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\HP\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1f0ced5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "911dfa90",
   "metadata": {},
   "outputs": [],
   "source": [
    "antonyms=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f8a71d7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['synonym', 'alternate']\n"
     ]
    }
   ],
   "source": [
    "for syn in wordnet.synsets(\"opposite\"):\n",
    "    for l in syn.lemmas():\n",
    "        if l.antonyms():\n",
    "            antonyms.append(l.antonyms()[0].name())   \n",
    "print(antonyms)            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "198ef0c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import brown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ee824670",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package brown to\n",
      "[nltk_data]     C:\\Users\\HP\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package brown is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('brown')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "65b2b63d",
   "metadata": {},
   "outputs": [],
   "source": [
    "w=brown.words()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e400dcbb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1161192"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a2dcdd54",
   "metadata": {},
   "outputs": [],
   "source": [
    "synonyms=[]\n",
    "antonyms=[]\n",
    "\n",
    "for syn in wordnet.synsets(\"good\"):\n",
    "    for l in syn.lemmas():\n",
    "        synonyms.append(l.name())\n",
    "        if l.antonyms():\n",
    "            antonyms.append(l.antonyms()[0].name())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "30e9e693",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'undecomposed', 'thoroughly', 'practiced', 'honest', 'skillful', 'upright', 'unspoiled', 'goodness', 'ripe', 'soundly', 'dependable', 'adept', 'safe', 'expert', 'proficient', 'near', 'honorable', 'effective', 'just', 'respectable', 'skilful', 'good', 'estimable', 'commodity', 'dear', 'unspoilt', 'secure', 'right', 'sound', 'in_force', 'full', 'trade_good', 'well', 'in_effect', 'salutary', 'serious', 'beneficial'}\n",
      "------------------------------------------------------------\n",
      "{'evil', 'ill', 'badness', 'evilness', 'bad'}\n"
     ]
    }
   ],
   "source": [
    "print(set(synonyms))\n",
    "print(\"------------------------------------------------------------\")\n",
    "print(set(antonyms))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8e704b80",
   "metadata": {},
   "outputs": [],
   "source": [
    "syns=wordnet.synsets(\"Computer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7520afb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calculator.n.01\n"
     ]
    }
   ],
   "source": [
    "print(syns[1].name())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "baf991d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "computer : \n",
      " a machine for performing calculations automatically \n",
      " []\n",
      "calculator : \n",
      " an expert at calculation (or at operating calculating machines) \n",
      " []\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(syns)):\n",
    "    print(syns[i].lemmas()[0].name(),\":\",\"\\n\",syns[i].definition(),\"\\n\",syns[i].examples(), end=\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "172de2fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on method lemmas in module nltk.corpus.reader.wordnet:\n",
      "\n",
      "lemmas(lemma, pos=None, lang='eng') method of nltk.corpus.reader.wordnet.WordNetCorpusReader instance\n",
      "    Return all Lemma objects with a name matching the specified lemma\n",
      "    name and part of speech tag. Matches any part of speech tag if none is\n",
      "    specified.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(wordnet.lemmas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "875f25ea",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'WordNetCorpusReader' object has no attribute 'antsets'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[35], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m syn_arr\u001b[38;5;241m=\u001b[39mwordnet\u001b[38;5;241m.\u001b[39mantsets(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbike\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      2\u001b[0m syn_arr[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mdefinition()\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'WordNetCorpusReader' object has no attribute 'antsets'"
     ]
    }
   ],
   "source": [
    "syn_arr=wordnet.antsets(\"bike\")\n",
    "syn_arr[0].definition()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "aa5195ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Synset('motorcycle.n.01'), Synset('bicycle.n.01'), Synset('bicycle.v.01')]\n"
     ]
    }
   ],
   "source": [
    "print(syn_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b3bd999f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ride a bicycle'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordnet.synset(\"bicycle.v.01\").definition()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d40c32c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['motorcycle', 'bike']"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordnet.synset(\"motorcycle.n.01\").lemma_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "71fc6eda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on WordNetCorpusReader in module nltk.corpus.reader.wordnet object:\n",
      "\n",
      "class WordNetCorpusReader(nltk.corpus.reader.api.CorpusReader)\n",
      " |  WordNetCorpusReader(root, omw_reader)\n",
      " |  \n",
      " |  A corpus reader used to access wordnet or its variants.\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      WordNetCorpusReader\n",
      " |      nltk.corpus.reader.api.CorpusReader\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, root, omw_reader)\n",
      " |      Construct a new wordnet corpus reader, with the given root\n",
      " |      directory.\n",
      " |  \n",
      " |  add_exomw(self)\n",
      " |      Add languages from Extended OMW\n",
      " |      \n",
      " |      >>> import nltk\n",
      " |      >>> from nltk.corpus import wordnet as wn\n",
      " |      >>> wn.add_exomw()\n",
      " |      >>> print(wn.synset('intrinsically.r.01').lemmas(lang=\"eng_wikt\"))\n",
      " |      [Lemma('intrinsically.r.01.per_se'), Lemma('intrinsically.r.01.as_such')]\n",
      " |  \n",
      " |  add_omw(self)\n",
      " |  \n",
      " |  add_provs(self, reader)\n",
      " |      Add languages from Multilingual Wordnet to the provenance dictionary\n",
      " |  \n",
      " |  all_eng_synsets(self, pos=None)\n",
      " |  \n",
      " |  all_lemma_names(self, pos=None, lang='eng')\n",
      " |      Return all lemma names for all synsets for the given\n",
      " |      part of speech tag and language or languages. If pos is\n",
      " |      not specified, all synsets for all parts of speech will\n",
      " |      be used.\n",
      " |  \n",
      " |  all_omw_synsets(self, pos=None, lang=None)\n",
      " |  \n",
      " |  all_synsets(self, pos=None, lang='eng')\n",
      " |      Iterate over all synsets with a given part of speech tag.\n",
      " |      If no pos is specified, all synsets for all parts of speech\n",
      " |      will be loaded.\n",
      " |  \n",
      " |  citation(self, lang='eng')\n",
      " |      Return the contents of citation.bib file (for omw)\n",
      " |      use lang=lang to get the citation for an individual language\n",
      " |  \n",
      " |  custom_lemmas(self, tab_file, lang)\n",
      " |      Reads a custom tab file containing mappings of lemmas in the given\n",
      " |      language to Princeton WordNet 3.0 synset offsets, allowing NLTK's\n",
      " |      WordNet functions to then be used with that language.\n",
      " |      \n",
      " |      See the \"Tab files\" section at https://omwn.org/omw1.html for\n",
      " |      documentation on the Multilingual WordNet tab file format.\n",
      " |      \n",
      " |      :param tab_file: Tab file as a file or file-like object\n",
      " |      :type: lang str\n",
      " |      :param: lang ISO 639-3 code of the language of the tab file\n",
      " |  \n",
      " |  digraph(self, inputs, rel=<function WordNetCorpusReader.<lambda> at 0x0000020C672EB560>, pos=None, maxdepth=-1, shapes=None, attr=None, verbose=False)\n",
      " |      Produce a graphical representation from 'inputs' (a list of\n",
      " |      start nodes, which can be a mix of Synsets, Lemmas and/or words),\n",
      " |      and a synset relation, for drawing with the 'dot' graph visualisation\n",
      " |      program from the Graphviz package.\n",
      " |      \n",
      " |      Return a string in the DOT graph file language, which can then be\n",
      " |      converted to an image by nltk.parse.dependencygraph.dot2img(dot_string).\n",
      " |      \n",
      " |      Optional Parameters:\n",
      " |      :rel: Wordnet synset relation\n",
      " |      :pos: for words, restricts Part of Speech to 'n', 'v', 'a' or 'r'\n",
      " |      :maxdepth: limit the longest path\n",
      " |      :shapes: dictionary of strings that trigger a specified shape\n",
      " |      :attr: dictionary with global graph attributes\n",
      " |      :verbose: warn about cycles\n",
      " |      \n",
      " |      >>> from nltk.corpus import wordnet as wn\n",
      " |      >>> print(wn.digraph([wn.synset('dog.n.01')]))\n",
      " |      digraph G {\n",
      " |      \"Synset('animal.n.01')\" -> \"Synset('organism.n.01')\";\n",
      " |      \"Synset('canine.n.02')\" -> \"Synset('carnivore.n.01')\";\n",
      " |      \"Synset('carnivore.n.01')\" -> \"Synset('placental.n.01')\";\n",
      " |      \"Synset('chordate.n.01')\" -> \"Synset('animal.n.01')\";\n",
      " |      \"Synset('dog.n.01')\" -> \"Synset('canine.n.02')\";\n",
      " |      \"Synset('dog.n.01')\" -> \"Synset('domestic_animal.n.01')\";\n",
      " |      \"Synset('domestic_animal.n.01')\" -> \"Synset('animal.n.01')\";\n",
      " |      \"Synset('living_thing.n.01')\" -> \"Synset('whole.n.02')\";\n",
      " |      \"Synset('mammal.n.01')\" -> \"Synset('vertebrate.n.01')\";\n",
      " |      \"Synset('object.n.01')\" -> \"Synset('physical_entity.n.01')\";\n",
      " |      \"Synset('organism.n.01')\" -> \"Synset('living_thing.n.01')\";\n",
      " |      \"Synset('physical_entity.n.01')\" -> \"Synset('entity.n.01')\";\n",
      " |      \"Synset('placental.n.01')\" -> \"Synset('mammal.n.01')\";\n",
      " |      \"Synset('vertebrate.n.01')\" -> \"Synset('chordate.n.01')\";\n",
      " |      \"Synset('whole.n.02')\" -> \"Synset('object.n.01')\";\n",
      " |      }\n",
      " |      <BLANKLINE>\n",
      " |  \n",
      " |  disable_custom_lemmas(self, lang)\n",
      " |      prevent synsets from being mistakenly added\n",
      " |  \n",
      " |  doc(self, file='README', lang='eng')\n",
      " |      Return the contents of readme, license or citation file\n",
      " |      use lang=lang to get the file for an individual language\n",
      " |  \n",
      " |  get_version(self)\n",
      " |  \n",
      " |  ic(self, corpus, weight_senses_equally=False, smoothing=1.0)\n",
      " |      Creates an information content lookup dictionary from a corpus.\n",
      " |      \n",
      " |      :type corpus: CorpusReader\n",
      " |      :param corpus: The corpus from which we create an information\n",
      " |          content dictionary.\n",
      " |      :type weight_senses_equally: bool\n",
      " |      :param weight_senses_equally: If this is True, gives all\n",
      " |          possible senses equal weight rather than dividing by the\n",
      " |          number of possible senses.  (If a word has 3 synses, each\n",
      " |          sense gets 0.3333 per appearance when this is False, 1.0 when\n",
      " |          it is true.)\n",
      " |      :param smoothing: How much do we smooth synset counts (default is 1.0)\n",
      " |      :type smoothing: float\n",
      " |      :return: An information content dictionary\n",
      " |  \n",
      " |  index_sense(self, version=None)\n",
      " |      Read sense key to synset id mapping from index.sense file in corpus directory\n",
      " |  \n",
      " |  jcn_similarity(self, synset1, synset2, ic, verbose=False)\n",
      " |      Jiang-Conrath Similarity:\n",
      " |      Return a score denoting how similar two word senses are, based on the\n",
      " |      Information Content (IC) of the Least Common Subsumer (most specific\n",
      " |      ancestor node) and that of the two input Synsets. The relationship is\n",
      " |      given by the equation 1 / (IC(s1) + IC(s2) - 2 * IC(lcs)).\n",
      " |      \n",
      " |      :type  other: Synset\n",
      " |      :param other: The ``Synset`` that this ``Synset`` is being compared to.\n",
      " |      :type  ic: dict\n",
      " |      :param ic: an information content object (as returned by\n",
      " |          ``nltk.corpus.wordnet_ic.ic()``).\n",
      " |      :return: A float score denoting the similarity of the two ``Synset``\n",
      " |          objects.\n",
      " |  \n",
      " |  langs(self)\n",
      " |      return a list of languages supported by Multilingual Wordnet\n",
      " |  \n",
      " |  lch_similarity(self, synset1, synset2, verbose=False, simulate_root=True)\n",
      " |      Leacock Chodorow Similarity:\n",
      " |      Return a score denoting how similar two word senses are, based on the\n",
      " |      shortest path that connects the senses (as above) and the maximum depth\n",
      " |      of the taxonomy in which the senses occur. The relationship is given as\n",
      " |      -log(p/2d) where p is the shortest path length and d is the taxonomy\n",
      " |      depth.\n",
      " |      \n",
      " |      :type  other: Synset\n",
      " |      :param other: The ``Synset`` that this ``Synset`` is being compared to.\n",
      " |      :type simulate_root: bool\n",
      " |      :param simulate_root: The various verb taxonomies do not\n",
      " |          share a single root which disallows this metric from working for\n",
      " |          synsets that are not connected. This flag (True by default)\n",
      " |          creates a fake root that connects all the taxonomies. Set it\n",
      " |          to false to disable this behavior. For the noun taxonomy,\n",
      " |          there is usually a default root except for WordNet version 1.6.\n",
      " |          If you are using wordnet 1.6, a fake root will be added for nouns\n",
      " |          as well.\n",
      " |      :return: A score denoting the similarity of the two ``Synset`` objects,\n",
      " |          normally greater than 0. None is returned if no connecting path\n",
      " |          could be found. If a ``Synset`` is compared with itself, the\n",
      " |          maximum score is returned, which varies depending on the taxonomy\n",
      " |          depth.\n",
      " |  \n",
      " |  lemma(self, name, lang='eng')\n",
      " |      Return lemma object that matches the name\n",
      " |  \n",
      " |  lemma_count(self, lemma)\n",
      " |      Return the frequency count for this Lemma\n",
      " |  \n",
      " |  lemma_from_key(self, key)\n",
      " |  \n",
      " |  lemmas(self, lemma, pos=None, lang='eng')\n",
      " |      Return all Lemma objects with a name matching the specified lemma\n",
      " |      name and part of speech tag. Matches any part of speech tag if none is\n",
      " |      specified.\n",
      " |  \n",
      " |  license(self, lang='eng')\n",
      " |      Return the contents of LICENSE (for omw)\n",
      " |      use lang=lang to get the license for an individual language\n",
      " |  \n",
      " |  lin_similarity(self, synset1, synset2, ic, verbose=False)\n",
      " |      Lin Similarity:\n",
      " |      Return a score denoting how similar two word senses are, based on the\n",
      " |      Information Content (IC) of the Least Common Subsumer (most specific\n",
      " |      ancestor node) and that of the two input Synsets. The relationship is\n",
      " |      given by the equation 2 * IC(lcs) / (IC(s1) + IC(s2)).\n",
      " |      \n",
      " |      :type other: Synset\n",
      " |      :param other: The ``Synset`` that this ``Synset`` is being compared to.\n",
      " |      :type ic: dict\n",
      " |      :param ic: an information content object (as returned by\n",
      " |          ``nltk.corpus.wordnet_ic.ic()``).\n",
      " |      :return: A float score denoting the similarity of the two ``Synset``\n",
      " |          objects, in the range 0 to 1.\n",
      " |  \n",
      " |  map_to_many(self)\n",
      " |  \n",
      " |  map_to_one(self)\n",
      " |  \n",
      " |  map_wn30(self)\n",
      " |      Mapping from Wordnet 3.0 to currently loaded Wordnet version\n",
      " |  \n",
      " |  morphy(self, form, pos=None, check_exceptions=True)\n",
      " |      Find a possible base form for the given form, with the given\n",
      " |      part of speech, by checking WordNet's list of exceptional\n",
      " |      forms, and by recursively stripping affixes for this part of\n",
      " |      speech until a form in WordNet is found.\n",
      " |      \n",
      " |      >>> from nltk.corpus import wordnet as wn\n",
      " |      >>> print(wn.morphy('dogs'))\n",
      " |      dog\n",
      " |      >>> print(wn.morphy('churches'))\n",
      " |      church\n",
      " |      >>> print(wn.morphy('aardwolves'))\n",
      " |      aardwolf\n",
      " |      >>> print(wn.morphy('abaci'))\n",
      " |      abacus\n",
      " |      >>> wn.morphy('hardrock', wn.ADV)\n",
      " |      >>> print(wn.morphy('book', wn.NOUN))\n",
      " |      book\n",
      " |      >>> wn.morphy('book', wn.ADJ)\n",
      " |  \n",
      " |  of2ss(self, of)\n",
      " |      take an id and return the synsets\n",
      " |  \n",
      " |  path_similarity(self, synset1, synset2, verbose=False, simulate_root=True)\n",
      " |      Path Distance Similarity:\n",
      " |      Return a score denoting how similar two word senses are, based on the\n",
      " |      shortest path that connects the senses in the is-a (hypernym/hypnoym)\n",
      " |      taxonomy. The score is in the range 0 to 1, except in those cases where\n",
      " |      a path cannot be found (will only be true for verbs as there are many\n",
      " |      distinct verb taxonomies), in which case None is returned. A score of\n",
      " |      1 represents identity i.e. comparing a sense with itself will return 1.\n",
      " |      \n",
      " |      :type other: Synset\n",
      " |      :param other: The ``Synset`` that this ``Synset`` is being compared to.\n",
      " |      :type simulate_root: bool\n",
      " |      :param simulate_root: The various verb taxonomies do not\n",
      " |          share a single root which disallows this metric from working for\n",
      " |          synsets that are not connected. This flag (True by default)\n",
      " |          creates a fake root that connects all the taxonomies. Set it\n",
      " |          to false to disable this behavior. For the noun taxonomy,\n",
      " |          there is usually a default root except for WordNet version 1.6.\n",
      " |          If you are using wordnet 1.6, a fake root will be added for nouns\n",
      " |          as well.\n",
      " |      :return: A score denoting the similarity of the two ``Synset`` objects,\n",
      " |          normally between 0 and 1. None is returned if no connecting path\n",
      " |          could be found. 1 is returned if a ``Synset`` is compared with\n",
      " |          itself.\n",
      " |  \n",
      " |  readme(self, lang='eng')\n",
      " |      Return the contents of README (for omw)\n",
      " |      use lang=lang to get the readme for an individual language\n",
      " |  \n",
      " |  res_similarity(self, synset1, synset2, ic, verbose=False)\n",
      " |      Resnik Similarity:\n",
      " |      Return a score denoting how similar two word senses are, based on the\n",
      " |      Information Content (IC) of the Least Common Subsumer (most specific\n",
      " |      ancestor node).\n",
      " |      \n",
      " |      :type  other: Synset\n",
      " |      :param other: The ``Synset`` that this ``Synset`` is being compared to.\n",
      " |      :type ic: dict\n",
      " |      :param ic: an information content object (as returned by\n",
      " |          ``nltk.corpus.wordnet_ic.ic()``).\n",
      " |      :return: A float score denoting the similarity of the two ``Synset``\n",
      " |          objects. Synsets whose LCS is the root node of the taxonomy will\n",
      " |          have a score of 0 (e.g. N['dog'][0] and N['table'][0]).\n",
      " |  \n",
      " |  ss2of(self, ss)\n",
      " |      return the ID of the synset\n",
      " |  \n",
      " |  synonyms(self, word, lang='eng')\n",
      " |      return nested list with the synonyms of the different senses of word in the given language\n",
      " |  \n",
      " |  synset(self, name)\n",
      " |      #############################################################\n",
      " |      # Loading Synsets\n",
      " |      #############################################################\n",
      " |  \n",
      " |  synset_from_pos_and_offset(self, pos, offset)\n",
      " |      - pos: The synset's part of speech, matching one of the module level\n",
      " |        attributes ADJ, ADJ_SAT, ADV, NOUN or VERB ('a', 's', 'r', 'n', or 'v').\n",
      " |      - offset: The byte offset of this synset in the WordNet dict file\n",
      " |        for this pos.\n",
      " |      \n",
      " |      >>> from nltk.corpus import wordnet as wn\n",
      " |      >>> print(wn.synset_from_pos_and_offset('n', 1740))\n",
      " |      Synset('entity.n.01')\n",
      " |  \n",
      " |  synset_from_sense_key(self, sense_key)\n",
      " |      Retrieves synset based on a given sense_key. Sense keys can be\n",
      " |      obtained from lemma.key()\n",
      " |      \n",
      " |      From https://wordnet.princeton.edu/documentation/senseidx5wn:\n",
      " |      A sense_key is represented as::\n",
      " |      \n",
      " |          lemma % lex_sense (e.g. 'dog%1:18:01::')\n",
      " |      \n",
      " |      where lex_sense is encoded as::\n",
      " |      \n",
      " |          ss_type:lex_filenum:lex_id:head_word:head_id\n",
      " |      \n",
      " |      :lemma:       ASCII text of word/collocation, in lower case\n",
      " |      :ss_type:     synset type for the sense (1 digit int)\n",
      " |                    The synset type is encoded as follows::\n",
      " |      \n",
      " |                        1    NOUN\n",
      " |                        2    VERB\n",
      " |                        3    ADJECTIVE\n",
      " |                        4    ADVERB\n",
      " |                        5    ADJECTIVE SATELLITE\n",
      " |      :lex_filenum: name of lexicographer file containing the synset for the sense (2 digit int)\n",
      " |      :lex_id:      when paired with lemma, uniquely identifies a sense in the lexicographer file (2 digit int)\n",
      " |      :head_word:   lemma of the first word in satellite's head synset\n",
      " |                    Only used if sense is in an adjective satellite synset\n",
      " |      :head_id:     uniquely identifies sense in a lexicographer file when paired with head_word\n",
      " |                    Only used if head_word is present (2 digit int)\n",
      " |      \n",
      " |      >>> import nltk\n",
      " |      >>> from nltk.corpus import wordnet as wn\n",
      " |      >>> print(wn.synset_from_sense_key(\"drive%1:04:03::\"))\n",
      " |      Synset('drive.n.06')\n",
      " |      \n",
      " |      >>> print(wn.synset_from_sense_key(\"driving%1:04:03::\"))\n",
      " |      Synset('drive.n.06')\n",
      " |  \n",
      " |  synsets(self, lemma, pos=None, lang='eng', check_exceptions=True)\n",
      " |      Load all synsets with a given lemma and part of speech tag.\n",
      " |      If no pos is specified, all synsets for all parts of speech\n",
      " |      will be loaded.\n",
      " |      If lang is specified, all the synsets associated with the lemma name\n",
      " |      of that language will be returned.\n",
      " |  \n",
      " |  words(self, lang='eng')\n",
      " |      return lemmas of the given language as list of words\n",
      " |  \n",
      " |  wup_similarity(self, synset1, synset2, verbose=False, simulate_root=True)\n",
      " |      Wu-Palmer Similarity:\n",
      " |      Return a score denoting how similar two word senses are, based on the\n",
      " |      depth of the two senses in the taxonomy and that of their Least Common\n",
      " |      Subsumer (most specific ancestor node). Previously, the scores computed\n",
      " |      by this implementation did _not_ always agree with those given by\n",
      " |      Pedersen's Perl implementation of WordNet Similarity. However, with\n",
      " |      the addition of the simulate_root flag (see below), the score for\n",
      " |      verbs now almost always agree but not always for nouns.\n",
      " |      \n",
      " |      The LCS does not necessarily feature in the shortest path connecting\n",
      " |      the two senses, as it is by definition the common ancestor deepest in\n",
      " |      the taxonomy, not closest to the two senses. Typically, however, it\n",
      " |      will so feature. Where multiple candidates for the LCS exist, that\n",
      " |      whose shortest path to the root node is the longest will be selected.\n",
      " |      Where the LCS has multiple paths to the root, the longer path is used\n",
      " |      for the purposes of the calculation.\n",
      " |      \n",
      " |      :type  other: Synset\n",
      " |      :param other: The ``Synset`` that this ``Synset`` is being compared to.\n",
      " |      :type simulate_root: bool\n",
      " |      :param simulate_root: The various verb taxonomies do not\n",
      " |          share a single root which disallows this metric from working for\n",
      " |          synsets that are not connected. This flag (True by default)\n",
      " |          creates a fake root that connects all the taxonomies. Set it\n",
      " |          to false to disable this behavior. For the noun taxonomy,\n",
      " |          there is usually a default root except for WordNet version 1.6.\n",
      " |          If you are using wordnet 1.6, a fake root will be added for nouns\n",
      " |          as well.\n",
      " |      :return: A float score denoting the similarity of the two ``Synset``\n",
      " |          objects, normally greater than zero. If no connecting path between\n",
      " |          the two senses can be found, None is returned.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes defined here:\n",
      " |  \n",
      " |  ADJ = 'a'\n",
      " |  \n",
      " |  ADJ_SAT = 's'\n",
      " |  \n",
      " |  ADV = 'r'\n",
      " |  \n",
      " |  MORPHOLOGICAL_SUBSTITUTIONS = {'a': [('er', ''), ('est', ''), ('er', '...\n",
      " |  \n",
      " |  NOUN = 'n'\n",
      " |  \n",
      " |  VERB = 'v'\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from nltk.corpus.reader.api.CorpusReader:\n",
      " |  \n",
      " |  __repr__(self)\n",
      " |      Return repr(self).\n",
      " |  \n",
      " |  abspath(self, fileid)\n",
      " |      Return the absolute path for the given file.\n",
      " |      \n",
      " |      :type fileid: str\n",
      " |      :param fileid: The file identifier for the file whose path\n",
      " |          should be returned.\n",
      " |      :rtype: PathPointer\n",
      " |  \n",
      " |  abspaths(self, fileids=None, include_encoding=False, include_fileid=False)\n",
      " |      Return a list of the absolute paths for all fileids in this corpus;\n",
      " |      or for the given list of fileids, if specified.\n",
      " |      \n",
      " |      :type fileids: None or str or list\n",
      " |      :param fileids: Specifies the set of fileids for which paths should\n",
      " |          be returned.  Can be None, for all fileids; a list of\n",
      " |          file identifiers, for a specified set of fileids; or a single\n",
      " |          file identifier, for a single file.  Note that the return\n",
      " |          value is always a list of paths, even if ``fileids`` is a\n",
      " |          single file identifier.\n",
      " |      \n",
      " |      :param include_encoding: If true, then return a list of\n",
      " |          ``(path_pointer, encoding)`` tuples.\n",
      " |      \n",
      " |      :rtype: list(PathPointer)\n",
      " |  \n",
      " |  encoding(self, file)\n",
      " |      Return the unicode encoding for the given corpus file, if known.\n",
      " |      If the encoding is unknown, or if the given file should be\n",
      " |      processed using byte strings (str), then return None.\n",
      " |  \n",
      " |  ensure_loaded(self)\n",
      " |      Load this corpus (if it has not already been loaded).  This is\n",
      " |      used by LazyCorpusLoader as a simple method that can be used to\n",
      " |      make sure a corpus is loaded -- e.g., in case a user wants to\n",
      " |      do help(some_corpus).\n",
      " |  \n",
      " |  fileids(self)\n",
      " |      Return a list of file identifiers for the fileids that make up\n",
      " |      this corpus.\n",
      " |  \n",
      " |  open(self, file)\n",
      " |      Return an open stream that can be used to read the given file.\n",
      " |      If the file's encoding is not None, then the stream will\n",
      " |      automatically decode the file's contents into unicode.\n",
      " |      \n",
      " |      :param file: The file identifier of the file to read.\n",
      " |  \n",
      " |  raw(self, fileids=None)\n",
      " |      :param fileids: A list specifying the fileids that should be used.\n",
      " |      :return: the given file(s) as a single string.\n",
      " |      :rtype: str\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Readonly properties inherited from nltk.corpus.reader.api.CorpusReader:\n",
      " |  \n",
      " |  root\n",
      " |      The directory where this corpus is stored.\n",
      " |      \n",
      " |      :type: PathPointer\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from nltk.corpus.reader.api.CorpusReader:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(wordnet)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ebefa92",
   "metadata": {},
   "source": [
    "# Stemming and Lemmatizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "c4b4ea07",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "ac7b9a41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "knive\n"
     ]
    }
   ],
   "source": [
    "stemmer=PorterStemmer()\n",
    "print(stemmer.stem(\"inging\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "1cbf3b96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on method stem in module nltk.stem.porter:\n",
      "\n",
      "stem(word, to_lowercase=True) method of nltk.stem.porter.PorterStemmer instance\n",
      "    :param to_lowercase: if `to_lowercase=True` the word always lowercase\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(stemmer.stem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "65b99796",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "funnyli\n"
     ]
    }
   ],
   "source": [
    "stemmer=PorterStemmer()\n",
    "print(stemmer.stem(\"funnyly\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "11f6d7eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "knife\n"
     ]
    }
   ],
   "source": [
    "stemmer=PorterStemmer()\n",
    "print(stemmer.stem(\"knifes\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "bcad99c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "increas\n"
     ]
    }
   ],
   "source": [
    "stemmer=PorterStemmer()\n",
    "print(stemmer.stem(\"increases\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "bc3c28bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ing\n"
     ]
    }
   ],
   "source": [
    "stemmer=PorterStemmer()\n",
    "print(stemmer.stem(\"inging\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0dda7c6",
   "metadata": {},
   "source": [
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "0bc4c59f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "4c577ec1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class WordNetLemmatizer in module nltk.stem.wordnet:\n",
      "\n",
      "class WordNetLemmatizer(builtins.object)\n",
      " |  WordNet Lemmatizer\n",
      " |  \n",
      " |  Lemmatize using WordNet's built-in morphy function.\n",
      " |  Returns the input word unchanged if it cannot be found in WordNet.\n",
      " |  \n",
      " |      >>> from nltk.stem import WordNetLemmatizer\n",
      " |      >>> wnl = WordNetLemmatizer()\n",
      " |      >>> print(wnl.lemmatize('dogs'))\n",
      " |      dog\n",
      " |      >>> print(wnl.lemmatize('churches'))\n",
      " |      church\n",
      " |      >>> print(wnl.lemmatize('aardwolves'))\n",
      " |      aardwolf\n",
      " |      >>> print(wnl.lemmatize('abaci'))\n",
      " |      abacus\n",
      " |      >>> print(wnl.lemmatize('hardrock'))\n",
      " |      hardrock\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __repr__(self)\n",
      " |      Return repr(self).\n",
      " |  \n",
      " |  lemmatize(self, word: str, pos: str = 'n') -> str\n",
      " |      Lemmatize `word` using WordNet's built-in morphy function.\n",
      " |      Returns the input word unchanged if it cannot be found in WordNet.\n",
      " |      \n",
      " |      :param word: The input word to lemmatize.\n",
      " |      :type word: str\n",
      " |      :param pos: The Part Of Speech tag. Valid options are `\"n\"` for nouns,\n",
      " |          `\"v\"` for verbs, `\"a\"` for adjectives, `\"r\"` for adverbs and `\"s\"`\n",
      " |          for satellite adjectives.\n",
      " |      :param pos: str\n",
      " |      :return: The lemma of `word`, for the given `pos`.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors defined here:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(WordNetLemmatizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "6310472b",
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer=WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "bd7c1689",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "increase\n"
     ]
    }
   ],
   "source": [
    "print(lemmatizer.lemmatize(\"increases\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "86a12500",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inging\n"
     ]
    }
   ],
   "source": [
    "print(lemmatizer.lemmatize(\"inging\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "cb789555",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "knife\n"
     ]
    }
   ],
   "source": [
    "print(lemmatizer.lemmatize(\"knives\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "824addc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "knife\n"
     ]
    }
   ],
   "source": [
    "print(lemmatizer.lemmatize(\"knifes\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "72ba18df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fastly\n"
     ]
    }
   ],
   "source": [
    "print(lemmatizer.lemmatize(\"fastly\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "e0068f35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wife\n"
     ]
    }
   ],
   "source": [
    "print(lemmatizer.lemmatize(\"wives\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "40eb5d81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fasting\n"
     ]
    }
   ],
   "source": [
    "print(lemmatizer.lemmatize(\"fasting\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5868ef5b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
